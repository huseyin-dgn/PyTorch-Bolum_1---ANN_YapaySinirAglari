{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1038eb2c",
   "metadata": {},
   "source": [
    "# PyTorch Modelini İleri Seviyeye Geliştirme (Advanced Enhancements)\n",
    "Bu kısımda CleanEnhancedANN modelimize ekleyebileceğimiz gelişmiş özellikleri adım adım inceleyeceğiz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb83e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 1️⃣ Farklı Aktivasyon Fonksiyonları\n",
    "\n",
    "Aktivasyon fonksiyonları, nöronların çıkış değerlerini dönüştürerek modelin **karmaşık ilişkileri öğrenmesini** sağlar.  \n",
    "Şu an modelimizde `LeakyReLU` kullanıyoruz, ama başka seçenekler de var:\n",
    "\n",
    "```python\n",
    "# ReLU\n",
    "relu = nn.ReLU()  \n",
    "# LeakyReLU\n",
    "leaky_relu = nn.LeakyReLU(0.1)  \n",
    "# ELU\n",
    "elu = nn.ELU(alpha=1.0)  \n",
    "# SELU\n",
    "selu = nn.SELU()  \n",
    "# Tanh\n",
    "tanh = nn.Tanh()  \n",
    "# Sigmoid\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aaf992",
   "metadata": {},
   "source": [
    "##  Aktivasyon Fonksiyonları – Kod ve Açıklama Tablosu\n",
    "\n",
    "| Aktivasyon | Kod | Açıklama |\n",
    "|------------|-----|----------|\n",
    "| ReLU | `nn.ReLU()` | Çıkış = max(0, x). Basit ve hızlı, derin ağlarda yaygın. Dezavantaj: negatiflerde ölü nöron problemi. |\n",
    "| LeakyReLU | `nn.LeakyReLU(0.1)` | Çıkış = x (x>0), 0.1*x (x<0). Negatiflerde küçük gradyan, ölü nöron sorununu azaltır. Mevcut modelde kullanılıyor. |\n",
    "| ELU | `nn.ELU(alpha=1.0)` | x>0 → x, x<=0 → alpha*(exp(x)-1). Negatif değerlerde gradyan verir, öğrenme stabilitesini artırır. Derin ağlarda ReLU’dan daha iyi performans gösterebilir. |\n",
    "| SELU | `nn.SELU()` | Kendiliğinden normalizasyon sağlar (self-normalizing). Derin ağlarda gradyan kaybını ve patlamasını azaltır. Özellikle katman sayısı fazla modeller için güçlü. |\n",
    "| Tanh | `nn.Tanh()` | Çıkış aralığı [-1, 1]. Sinyali normalize eder. Dezavantaj: çok derin ağlarda vanishing gradient problemi yaşanabilir. |\n",
    "| Sigmoid | `nn.Sigmoid()` | Çıkış aralığı [0,1]. Genellikle çıkış katmanı için kullanılır (binary classification). Dezavantaj: çok derin katmanlarda gradyan kaybı olabilir. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed6897",
   "metadata": {},
   "source": [
    "## 2️⃣ Aktivasyon Fonksiyonları – Kullanım Senaryoları\n",
    "\n",
    "| Aktivasyon | Kullanım Durumu / İhtimal | Notlar |\n",
    "|------------|---------------------------|--------|\n",
    "| ReLU | Genel derin ağlar, hızlı ve basit modeller | Negatif değerleri sıfırladığı için ölü nöron problemi olabilir |\n",
    "| LeakyReLU | Derin ağlar, ölü nöron riskini azaltmak istediğinde | Küçük negatif slope ile negatif gradyan sağlar, mevcut modelde kullanılıyor |\n",
    "| ELU | Derin ağlar, daha stabil öğrenme ve negatif değerlerin önemli olduğu durumlar | ReLU’dan daha iyi performans gösterebilir, küçük negatif gradyan verir |\n",
    "| SELU | Çok derin katmanlı ağlar, self-normalizing gereken durumlar | Otomatik normalize eder, gradyan kaybı/patlamasını azaltır |\n",
    "| Tanh | Giriş ve çıkış değerlerini normalize etmek istediğinde, sinyaller [-1,1] aralığında olmalı | Vanishing gradient riski derin ağlarda yüksek |\n",
    "| Sigmoid | Binary classification çıkış katmanı, olasılık tahmini | Derin katmanlarda gradyan kaybı olabilir, genellikle sadece çıkış katmanında kullanılır |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803898ce",
   "metadata": {},
   "source": [
    "## 🔹 Aktivasyon Fonksiyonları – Ek Notlar ve İpuçları\n",
    "\n",
    "1. **Çıkış Katmanı ile Uyumluluk**\n",
    "   - `Sigmoid` → binary classification için çıkış katmanı\n",
    "   - `Softmax` → multi-class classification için çıkış katmanı\n",
    "   - Gizli katmanda Softmax kullanmak genellikle önerilmez\n",
    "\n",
    "2. **Derinlik ve Gradyan Problemleri**\n",
    "   - ReLU ve LeakyReLU → gradyan kaybını (vanishing gradient) azaltır\n",
    "   - Tanh ve Sigmoid → derin ağlarda gradyan kaybına yol açabilir, dikkatli kullanılmalı\n",
    "\n",
    "3. **Performans ve Hız**\n",
    "   - ReLU → en hızlı, GPU’larda optimize edilmiş\n",
    "   - ELU / SELU → biraz daha yavaş ama stabilite ve performans artışı sağlar\n",
    "\n",
    "4. **Kendi Aktivasyonunu Oluşturma**\n",
    "   - `nn.Module` veya `torch.autograd.Function` ile custom activation yazılabilir\n",
    "   - Örnek: negatif değerleri özel bir şekilde ölçeklendirmek veya yeni non-linearity denemek\n",
    "\n",
    "5. **Kombinasyonlar**\n",
    "   - İleri modellerde birden fazla aktivasyon bir arada kullanılabilir\n",
    "   - Örnek: ilk katman ReLU, ikinci katman ELU → stabilite ve hız dengesi sağlanabilir\n",
    "\n",
    "\n",
    "\n",
    "💡 **Özet:**  \n",
    "- Aktivasyon seçimi **gizli katman vs çıkış katmanı** ve **veri tipi / problem tipi** ile doğrudan ilişkili.  \n",
    "- Çıkış katmanı için problem türüne uygun aktivasyon seçmek kritik.  \n",
    "- Derinlik ve gradyan problemleri göz önünde bulundurulmalı.  \n",
    "- İleri seviyede **custom veya kombinasyonlu aktivasyonlar** deneyebilirsin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee14f42",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a717b",
   "metadata": {},
   "source": [
    "# 2️⃣ Residual (Skip) Bağlantıları ve Normalizasyon\n",
    "\n",
    "Bu bölümde modelin stabilitesini ve öğrenme kapasitesini artıracak ileri seviye geliştirmeleri inceleyeceğiz.\n",
    "\n",
    "\n",
    "\n",
    "## 🔹 Residual (Skip) Bağlantıları\n",
    "\n",
    "- Amaç: Derin ağlarda **gradyan kaybını önlemek** ve öğrenmeyi hızlandırmak\n",
    "- Mantık: Bir katmanın çıktısını bir sonraki katmana **direkt ekler**\n",
    "- Örnek:\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(size, size),\n",
    "            nn.BatchNorm1d(size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31a484",
   "metadata": {},
   "source": [
    "* Residual bağlantıları diğer konularda daha net anlayacağız.ANN için bu kadar örnek yeterlidir diye düşünüyorum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3fa2b",
   "metadata": {},
   "source": [
    "## 🔹 Batch Normalization (BatchNorm)\n",
    "\n",
    "- **Amaç:** Katman çıkışlarını normalize ederek eğitimi stabil ve hızlı hale getirir\n",
    "\n",
    "- **Örnek:**\n",
    "```python\n",
    "bn = nn.BatchNorm1d(64)  # 64 nöronlu bir katman için\n",
    "x = bn(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b10d53",
   "metadata": {},
   "source": [
    "## 🔹 Layer Normalization (LayerNorm)\n",
    "\n",
    "- **Amaç:** Küçük batch veya sequence verilerde stabilite sağlamak\n",
    "\n",
    "- **Örnek:**\n",
    "```python\n",
    "ln = nn.LayerNorm(64)\n",
    "x = ln(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8b3ab",
   "metadata": {},
   "source": [
    "Notlar:\n",
    "\n",
    "* Batch boyutuna bağımlı değildir\n",
    "\n",
    "* NLP ve sequence modellerinde sıklıkla tercih edilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757f870",
   "metadata": {},
   "source": [
    "---\n",
    "# 3️⃣ Weight Initialization ve Regularization\n",
    "\n",
    "Bu bölümde modelin **öğrenme hızı, stabilitesi ve overfitting kontrolü** için kullanılabilecek teknikleri inceleyeceğiz.\n",
    "\n",
    "\n",
    "\n",
    "## 🔹 Weight Initialization\n",
    "\n",
    "- Amaç: Ağın ağırlıklarını uygun bir şekilde başlatarak **hızlı ve stabil öğrenme** sağlamak\n",
    "- Örnekler:\n",
    "\n",
    "```python\n",
    "# Xavier uniform initialization\n",
    "nn.init.xavier_uniform_(layer.weight)\n",
    "nn.init.zeros_(layer.bias)\n",
    "\n",
    "# Kaiming normal initialization (ReLU tabanlı ağlar için)\n",
    "nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "nn.init.zeros_(layer.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb87759",
   "metadata": {},
   "source": [
    "Notlar:\n",
    "\n",
    "* Xavier → genellikle sigmoid / tanh tabanlı ağlarda iyi\n",
    "\n",
    "* Kaiming → ReLU / LeakyReLU tabanlı ağlarda daha iyi performans\n",
    "\n",
    "* Orthogonal initialization → derin ağlarda gradyan stabilitesi sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3ac28",
   "metadata": {},
   "source": [
    "## 🔹 DropConnect\n",
    "\n",
    "Amaç: Dropout’a benzer ama ağırlıkları rastgele kapatır\n",
    "\n",
    "Örnek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "dropconnect = nn.Dropout(p=0.2)  # PyTorch'da DropConnect için custom layer yazılabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48190abd",
   "metadata": {},
   "source": [
    "Notlar:\n",
    "\n",
    "* Daha agresif regularization sağlar\n",
    "\n",
    "* Küçük datasetlerde overfitting’i azaltmak için tercih edilebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0ca9f",
   "metadata": {},
   "source": [
    "## 🔹 Özet Tablosu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9aa54d",
   "metadata": {},
   "source": [
    "| Teknik                 | Amaç                                                  | Kullanım / Notlar                                |\n",
    "| ---------------------- | ----------------------------------------------------- | ------------------------------------------------ |\n",
    "| Xavier Initialization  | Stabil öğrenme, hızlı konverjans                      | Sigmoid / Tanh tabanlı ağlar                     |\n",
    "| Kaiming Initialization | ReLU tabanlı ağlarda iyi performans                   | ReLU / LeakyReLU tabanlı ağlar                   |\n",
    "| Dropout                | Overfitting’i azaltmak                                | Eğitim sırasında bazı nöronları rastgele kapatır |\n",
    "| DropConnect            | Ağırlıkları rastgele kapatarak overfitting’i azaltmak | Özellikle küçük datasetlerde faydalı             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366b803",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4️⃣ Learning Rate ve Scheduler’lar\n",
    "\n",
    "Learning Rate (LR) modelin öğrenme hızını belirler ve doğru ayarlanmazsa eğitim **çok yavaş veya kararsız** olabilir.  \n",
    "Scheduler’lar, LR’yi dinamik olarak değiştirerek konverjansı hızlandırır ve stabil hale getirir.\n",
    "\n",
    "\n",
    "\n",
    "## 🔹 Sabit Learning Rate\n",
    "```python\n",
    "lr = 0.001  # Örnek sabit learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded142a",
   "metadata": {},
   "source": [
    "* Basit, küçük ağlarda genellikle yeterli\n",
    "\n",
    "* Derin ve kompleks ağlarda bazen stabilite sorunları yaşanabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675116a",
   "metadata": {},
   "source": [
    "## 🔹 StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2be6c65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m scheduler = torch.optim.lr_scheduler.StepLR(\u001b[43moptimizer\u001b[49m, step_size=\u001b[32m10\u001b[39m, gamma=\u001b[32m0.1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fdd5f",
   "metadata": {},
   "source": [
    "* Belirli epoch sayısı sonrası LR’yi çarpan (gamma) ile azaltır\n",
    "\n",
    "* Örnek: Her 10 epoch’ta LR 0.1 ile çarpılır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0c309",
   "metadata": {},
   "source": [
    "## 🔹 ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe4a36",
   "metadata": {},
   "source": [
    "* Her epoch’ta LR üstel olarak azalır\n",
    "\n",
    "* Kararlı ve yumuşak LR düşüşü sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311d976",
   "metadata": {},
   "source": [
    "## 🔹 ReduceLROnPlateau !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eef656",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbf9ec",
   "metadata": {},
   "source": [
    "* Validasyon kaybı belirli bir süre iyileşmezse LR’yi azaltır\n",
    "\n",
    "* Otomatik olarak LR ayarlamak için faydalı\n",
    "\n",
    "* Özellikle overfitting veya plateau durumlarında kullanılır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccb5bd",
   "metadata": {},
   "source": [
    "| Scheduler         | Amaç                           | Notlar                                        |\n",
    "| ----------------- | ------------------------------ | --------------------------------------------- |\n",
    "| Sabit LR          | Basit ve küçük ağlar           | Küçük ve stabil problem setlerinde yeterli    |\n",
    "| StepLR            | Belirli aralıklarla LR düşürme | Epoch tabanlı, gamma ile çarpılır             |\n",
    "| ExponentialLR     | Üstel LR düşüşü                | Daha yumuşak ve stabil düşüş sağlar           |\n",
    "| ReduceLROnPlateau | Plateau durumunda LR azaltma   | Validasyon kaybı iyileşmezse otomatik çalışır |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e58140",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# 5️⃣ Label Smoothing\n",
    "\n",
    "## 📌 Amaç\n",
    "- Modelin **aşırı güvenli tahminler yapmasını** engeller  \n",
    "- Cross-Entropy Loss yerine, sınıf dağılımını biraz \"yumuşatır\"  \n",
    "- Bu sayede **genelleme** artar ve **overfitting** azalır  \n",
    "\n",
    "\n",
    "\n",
    "## 🔹 Nasıl Çalışır?\n",
    "- Normalde doğru sınıf etiketi **%100 (1.0)** oluyordu  \n",
    "- Label smoothing ile → **%90 doğru sınıf, %10 diğer sınıflara dağıtılır**  \n",
    "- Örneğin:  \n",
    "  - Normal → `[0, 0, 1, 0]`  \n",
    "  - Smoothing (ε=0.1) → `[0.033, 0.033, 0.9, 0.033]`  \n",
    "\n",
    "\n",
    "\n",
    "## 🔹 Kod Örneği\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model çıkışı (örnek)\n",
    "outputs = torch.randn(4, 5)  # 4 örnek, 5 sınıf\n",
    "labels = torch.tensor([2, 1, 0, 3])  # gerçek etiketler\n",
    "\n",
    "# Label smoothing'li loss\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34129dc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Aşağıda bulunan modellerin bazıları örneklerde kullanılmıştır.Bu tür model kullanımlarına aşina olunuz.,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d6fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_MODEL(nn.Module):\n",
    "    def __init__(self, input_dim , hidden_layers=[128,64,32], dropout=0.3):\n",
    "        super(ANN_MODEL ,self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_layers:\n",
    "            self.layers.append(nn.Linear(in_dim,h_dim))\n",
    "            in_dim = h_dim\n",
    "\n",
    "        self.out = nn.Linear(in_dim,1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self , x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d4ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[256, 128, 64, 32], dropout=0.3):\n",
    "        super(AdvancedANN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_layers:\n",
    "            self.layers.append(nn.Linear(in_dim, h_dim))\n",
    "            in_dim = h_dim\n",
    "\n",
    "        self.out = nn.Linear(in_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x  # İlk input residual bağlantısı için\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Her 2 katmanda bir residual bağlantı ekle\n",
    "            if i % 2 == 1:\n",
    "                if residual.shape[1] == x.shape[1]:\n",
    "                    x = x + residual\n",
    "                residual = x  # residual'u güncelle\n",
    "            \n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size , output_size = 1, dp=0.3):\n",
    "        super(ANN ,self).__init__()\n",
    "\n",
    "        def linear_block(in_features , out_features , dropout):\n",
    "            return nn.Sequential(\n",
    "\n",
    "                nn.Linear(in_features , out_features),\n",
    "                nn.BatchNorm1d(out_features),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout)        \n",
    "                )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "                linear_block(input_size , 128 , dp),\n",
    "                linear_block(128,64,dp),\n",
    "                linear_block(64,32,dp),\n",
    "                nn.Linear(32,output_size)\n",
    "            )\n",
    "\n",
    "        for m in self.model:\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cd468",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
