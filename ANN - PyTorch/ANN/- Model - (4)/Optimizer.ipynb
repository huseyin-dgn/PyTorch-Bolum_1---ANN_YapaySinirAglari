{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fef9bc",
   "metadata": {},
   "source": [
    "# 5️⃣ Optimizer’lar\n",
    "\n",
    "Optimizer, modelin ağırlıklarını güncelleyen algoritmadır.  \n",
    "Doğru optimizer seçimi, **hızlı ve stabil öğrenme** için kritik öneme sahiptir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482e5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli importlar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Örnek model\n",
    "model = nn.Linear(10, 1)  # 10 giriş, 1 çıkış\n",
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ SGD (Stochastic Gradient Descent)\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 2️⃣ Adam\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3️⃣ AdamW (Adam with Weight Decay)\n",
    "optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# 4️⃣ RMSprop\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44547e92",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🔹 Stochastic Gradient Descent (SGD)\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3edca6",
   "metadata": {},
   "source": [
    "* Basit ve klasik optimizer\n",
    "\n",
    "* Momentum ile gradyan hızını artırır ve osilasyonu azaltır\n",
    "\n",
    "* Büyük veri setlerinde ve derin ağlarda bazen yavaş olabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6a7f6",
   "metadata": {},
   "source": [
    "## 🔹 Adam !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f13e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633083f",
   "metadata": {},
   "source": [
    "* Adaptive Moment Estimation\n",
    "\n",
    "* Öğrenme hızı otomatik ayarlanır (adaptive learning rate)\n",
    "\n",
    "* Derin ağlarda ve çoğu problemde en çok tercih edilen optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dad9ff",
   "metadata": {},
   "source": [
    "## 🔹 AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f288541",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969d09b",
   "metadata": {},
   "source": [
    "* Adam’ın L2 regularization ile geliştirilmiş versiyonu\n",
    "\n",
    "* Overfitting’i azaltmak için ağırlık çürümesini (weight decay) içerir\n",
    "\n",
    "* Transformer ve modern mimarilerde sık kullanılır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87d65f",
   "metadata": {},
   "source": [
    "## 🔹 RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ac83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783d0c8",
   "metadata": {},
   "source": [
    "* Adaptive learning rate kullanır\n",
    "\n",
    "* Özellikle RNN ve sequence modellerde iyi performans gösterir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f9c96",
   "metadata": {},
   "source": [
    "# 🔹 Özet Tablosu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ab1d6",
   "metadata": {},
   "source": [
    "| Optimizer | Amaç                                             | Kullanım / Notlar                                   |\n",
    "| --------- | ------------------------------------------------ | --------------------------------------------------- |\n",
    "| SGD       | Basit gradient descent, momentum ile hız artırır | Küçük ve orta boyutlu veri setleri, klasik kullanım |\n",
    "| Adam      | Adaptive learning rate, hızlı konverjans         | Derin ağlar ve çoğu problem için standart tercih    |\n",
    "| AdamW     | Adam + weight decay, overfitting kontrolü        | Transformer ve modern mimarilerde önerilir          |\n",
    "| RMSprop   | Adaptive LR, özellikle RNN                       | Sequence ve zaman serisi modellerinde tercih edilir |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3f6da",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "#  Optimizer ve İleri Seviye İpuçları\n",
    "\n",
    "Bu bölümde optimizer’lar, learning rate ve ileri seviye teknikler bir arada ele alınmıştır.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fc7e4",
   "metadata": {},
   "source": [
    "### 🔹 İleri Seviye Notlar\n",
    "\n",
    "#### 1. Momentum\n",
    "- Önceki gradyan adımlarını dikkate alır, osilasyonu azaltır.  \n",
    "- Adam, kendi adaptive moment mekanizmasını kullandığı için ekstra momentum genellikle gerekmez.\n",
    "\n",
    "#### 2. Weight Decay / L2 Regularization\n",
    "- `weight_decay` ile overfitting azaltılabilir  \n",
    "- AdamW, weight decay’i doğru uygular\n",
    "\n",
    "#### 3. Optimizer + Scheduler Kombinasyonu\n",
    "- Örnek: `Adam + ReduceLROnPlateau` veya `AdamW + StepLR`  \n",
    "- Scheduler, optimizer ile birlikte kullanılmalı\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b02f8",
   "metadata": {},
   "source": [
    "### 🔹 Cyclic Learning Rate (CLR)\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer_adam, base_lr=1e-4, max_lr=1e-2, step_size_up=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104b56e",
   "metadata": {},
   "source": [
    "* Learning rate’i periyodik olarak artırıp azaltır\n",
    "\n",
    "* Modelin plateau’da sıkışmasını önler ve konverjansı hızlandırır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ef0cb",
   "metadata": {},
   "source": [
    "### 🔹 Learning Rate Seçimi\n",
    "\n",
    "* Çok küçük LR → Model çok yavaş öğrenir\n",
    "\n",
    "* Çok büyük LR → Kayıp patlar ve eğitim kararsız olur\n",
    "\n",
    "Doğru LR seçimi optimizer performansını doğrudan etkiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1828092",
   "metadata": {},
   "source": [
    "### 🔹 Lookahead Optimizer\n",
    "\n",
    "- Daha **stabil öğrenme** sağlar  \n",
    "- PyTorch’da ek kütüphane ile uygulanabilir  \n",
    "- Derin ve karmaşık ağlarda **konverjans stabilitesini artırır**\n",
    "\n",
    "\n",
    "\n",
    "####  Nasıl Çalışır?\n",
    "\n",
    "1. **Fast weights (hızlı ağırlıklar):** Normal optimizer gibi her adımda ağırlıkları günceller.  \n",
    "2. **Slow weights (yavaş ağırlıklar):** Hızlı ağırlıkların belirli adım sayısı sonunda ortalamasını alır ve ağırlıkları daha stabil günceller.  \n",
    "3. **Faydası:** Ağırlıkların ani değişmesini engeller, derin ve karmaşık ağlarda konverjans daha kararlı olur.\n",
    "\n",
    "\n",
    "####  Örnek Kod\n",
    "\n",
    "```python\n",
    "# Ek kütüphaneyi yüklemeniz gerekir:\n",
    "# pip install torch-optimizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optimx\n",
    "\n",
    "# Örnek model\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# Base optimizer\n",
    "base_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lookahead optimizer\n",
    "optimizer = optimx.Lookahead(base_optimizer, k=5, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24d3ed",
   "metadata": {},
   "source": [
    "## 🔹 İleri Seviye Optimizer ve Learning Taktikleri – Ne Zaman Kullanılır?\n",
    "\n",
    "| Teknik / Taktik | Ne Zaman Kullanılır | Notlar / Avantajlar |\n",
    "|-----------------|------------------|------------------|\n",
    "| **Momentum (SGD)** | Ağ çok derin değil, ancak osilasyon veya yavaş öğrenme varsa | Gradyan hızını artırır, osilasyonu azaltır |\n",
    "| **Weight Decay / L2 Regularization** | Overfitting riski yüksek, özellikle küçük datasetlerde | Ağırlıkları küçülterek genelleme performansını artırır |\n",
    "| **Optimizer + Scheduler Kombinasyonu** | Derin veya karmaşık ağlarda, LR’yi dinamik ayarlamak istiyorsan | Plateau veya learning rate düşüşlerini otomatik uygular |\n",
    "| **Cyclic Learning Rate (CLR)** | Model plateau’da sıkışıyor, öğrenme yavaş veya LR ayarlamak zor | LR’i periyodik olarak değiştirir, hızlı konverjans sağlar |\n",
    "| **Lookahead Optimizer** | Çok derin veya karmaşık ağlar, gradyan stabilitesi sorunlu | Ağırlıkları yumuşatarak daha stabil konverjans sağlar |\n",
    "| **CLR + Scheduler Kombinasyonu** | Çok derin ağlarda ve büyük veri setlerinde | Öğrenme hızını dinamik tutarak eğitim stabilitesini ve hızını artırır |\n",
    "| **Adaptive LR Optimizer (Adam, AdamW, RMSprop)** | Standart derin ağlarda, çoğu problem | Learning rate’i otomatik ayarlar, çoğu durumda hızlı ve stabil öğrenme sağlar |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Özet\n",
    "\n",
    "- **Küçük ve basit ağlar:** SGD veya Adam genellikle yeterli  \n",
    "- **Derin ve karmaşık ağlar:** Scheduler, CLR ve Lookahead gibi ileri teknikler eklenmeli  \n",
    "- **Overfitting riski varsa:** Weight decay / L2 regularization + Dropout eklemek faydalı  \n",
    "- **Plateau veya stabilite sorunları varsa:** CLR, ReduceLROnPlateau ve Lookahead kullanmak en mantıklısı\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d729e5a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
