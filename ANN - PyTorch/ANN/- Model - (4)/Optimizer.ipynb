{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fef9bc",
   "metadata": {},
   "source": [
    "# 5ï¸âƒ£ Optimizerâ€™lar\n",
    "\n",
    "Optimizer, modelin aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncelleyen algoritmadÄ±r.  \n",
    "DoÄŸru optimizer seÃ§imi, **hÄ±zlÄ± ve stabil Ã¶ÄŸrenme** iÃ§in kritik Ã¶neme sahiptir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482e5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli importlar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Ã–rnek model\n",
    "model = nn.Linear(10, 1)  # 10 giriÅŸ, 1 Ã§Ä±kÄ±ÅŸ\n",
    "\n",
    "# ----------------------------\n",
    "# 1ï¸âƒ£ SGD (Stochastic Gradient Descent)\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 2ï¸âƒ£ Adam\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3ï¸âƒ£ AdamW (Adam with Weight Decay)\n",
    "optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# 4ï¸âƒ£ RMSprop\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44547e92",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ğŸ”¹ Stochastic Gradient Descent (SGD)\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3edca6",
   "metadata": {},
   "source": [
    "* Basit ve klasik optimizer\n",
    "\n",
    "* Momentum ile gradyan hÄ±zÄ±nÄ± artÄ±rÄ±r ve osilasyonu azaltÄ±r\n",
    "\n",
    "* BÃ¼yÃ¼k veri setlerinde ve derin aÄŸlarda bazen yavaÅŸ olabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6a7f6",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Adam !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f13e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633083f",
   "metadata": {},
   "source": [
    "* Adaptive Moment Estimation\n",
    "\n",
    "* Ã–ÄŸrenme hÄ±zÄ± otomatik ayarlanÄ±r (adaptive learning rate)\n",
    "\n",
    "* Derin aÄŸlarda ve Ã§oÄŸu problemde en Ã§ok tercih edilen optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dad9ff",
   "metadata": {},
   "source": [
    "## ğŸ”¹ AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f288541",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969d09b",
   "metadata": {},
   "source": [
    "* Adamâ€™Ä±n L2 regularization ile geliÅŸtirilmiÅŸ versiyonu\n",
    "\n",
    "* Overfittingâ€™i azaltmak iÃ§in aÄŸÄ±rlÄ±k Ã§Ã¼rÃ¼mesini (weight decay) iÃ§erir\n",
    "\n",
    "* Transformer ve modern mimarilerde sÄ±k kullanÄ±lÄ±r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87d65f",
   "metadata": {},
   "source": [
    "## ğŸ”¹ RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ac83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783d0c8",
   "metadata": {},
   "source": [
    "* Adaptive learning rate kullanÄ±r\n",
    "\n",
    "* Ã–zellikle RNN ve sequence modellerde iyi performans gÃ¶sterir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f9c96",
   "metadata": {},
   "source": [
    "# ğŸ”¹ Ã–zet Tablosu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ab1d6",
   "metadata": {},
   "source": [
    "| Optimizer | AmaÃ§                                             | KullanÄ±m / Notlar                                   |\n",
    "| --------- | ------------------------------------------------ | --------------------------------------------------- |\n",
    "| SGD       | Basit gradient descent, momentum ile hÄ±z artÄ±rÄ±r | KÃ¼Ã§Ã¼k ve orta boyutlu veri setleri, klasik kullanÄ±m |\n",
    "| Adam      | Adaptive learning rate, hÄ±zlÄ± konverjans         | Derin aÄŸlar ve Ã§oÄŸu problem iÃ§in standart tercih    |\n",
    "| AdamW     | Adam + weight decay, overfitting kontrolÃ¼        | Transformer ve modern mimarilerde Ã¶nerilir          |\n",
    "| RMSprop   | Adaptive LR, Ã¶zellikle RNN                       | Sequence ve zaman serisi modellerinde tercih edilir |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3f6da",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "#  Optimizer ve Ä°leri Seviye Ä°puÃ§larÄ±\n",
    "\n",
    "Bu bÃ¶lÃ¼mde optimizerâ€™lar, learning rate ve ileri seviye teknikler bir arada ele alÄ±nmÄ±ÅŸtÄ±r.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fc7e4",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Ä°leri Seviye Notlar\n",
    "\n",
    "#### 1. Momentum\n",
    "- Ã–nceki gradyan adÄ±mlarÄ±nÄ± dikkate alÄ±r, osilasyonu azaltÄ±r.  \n",
    "- Adam, kendi adaptive moment mekanizmasÄ±nÄ± kullandÄ±ÄŸÄ± iÃ§in ekstra momentum genellikle gerekmez.\n",
    "\n",
    "#### 2. Weight Decay / L2 Regularization\n",
    "- `weight_decay` ile overfitting azaltÄ±labilir  \n",
    "- AdamW, weight decayâ€™i doÄŸru uygular\n",
    "\n",
    "#### 3. Optimizer + Scheduler Kombinasyonu\n",
    "- Ã–rnek: `Adam + ReduceLROnPlateau` veya `AdamW + StepLR`  \n",
    "- Scheduler, optimizer ile birlikte kullanÄ±lmalÄ±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b02f8",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Cyclic Learning Rate (CLR)\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer_adam, base_lr=1e-4, max_lr=1e-2, step_size_up=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104b56e",
   "metadata": {},
   "source": [
    "* Learning rateâ€™i periyodik olarak artÄ±rÄ±p azaltÄ±r\n",
    "\n",
    "* Modelin plateauâ€™da sÄ±kÄ±ÅŸmasÄ±nÄ± Ã¶nler ve konverjansÄ± hÄ±zlandÄ±rÄ±r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ef0cb",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Learning Rate SeÃ§imi\n",
    "\n",
    "* Ã‡ok kÃ¼Ã§Ã¼k LR â†’ Model Ã§ok yavaÅŸ Ã¶ÄŸrenir\n",
    "\n",
    "* Ã‡ok bÃ¼yÃ¼k LR â†’ KayÄ±p patlar ve eÄŸitim kararsÄ±z olur\n",
    "\n",
    "DoÄŸru LR seÃ§imi optimizer performansÄ±nÄ± doÄŸrudan etkiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1828092",
   "metadata": {},
   "source": [
    "### ğŸ”¹ Lookahead Optimizer\n",
    "\n",
    "- Daha **stabil Ã¶ÄŸrenme** saÄŸlar  \n",
    "- PyTorchâ€™da ek kÃ¼tÃ¼phane ile uygulanabilir  \n",
    "- Derin ve karmaÅŸÄ±k aÄŸlarda **konverjans stabilitesini artÄ±rÄ±r**\n",
    "\n",
    "\n",
    "\n",
    "####  NasÄ±l Ã‡alÄ±ÅŸÄ±r?\n",
    "\n",
    "1. **Fast weights (hÄ±zlÄ± aÄŸÄ±rlÄ±klar):** Normal optimizer gibi her adÄ±mda aÄŸÄ±rlÄ±klarÄ± gÃ¼nceller.  \n",
    "2. **Slow weights (yavaÅŸ aÄŸÄ±rlÄ±klar):** HÄ±zlÄ± aÄŸÄ±rlÄ±klarÄ±n belirli adÄ±m sayÄ±sÄ± sonunda ortalamasÄ±nÄ± alÄ±r ve aÄŸÄ±rlÄ±klarÄ± daha stabil gÃ¼nceller.  \n",
    "3. **FaydasÄ±:** AÄŸÄ±rlÄ±klarÄ±n ani deÄŸiÅŸmesini engeller, derin ve karmaÅŸÄ±k aÄŸlarda konverjans daha kararlÄ± olur.\n",
    "\n",
    "\n",
    "####  Ã–rnek Kod\n",
    "\n",
    "```python\n",
    "# Ek kÃ¼tÃ¼phaneyi yÃ¼klemeniz gerekir:\n",
    "# pip install torch-optimizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optimx\n",
    "\n",
    "# Ã–rnek model\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# Base optimizer\n",
    "base_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lookahead optimizer\n",
    "optimizer = optimx.Lookahead(base_optimizer, k=5, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24d3ed",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Ä°leri Seviye Optimizer ve Learning Taktikleri â€“ Ne Zaman KullanÄ±lÄ±r?\n",
    "\n",
    "| Teknik / Taktik | Ne Zaman KullanÄ±lÄ±r | Notlar / Avantajlar |\n",
    "|-----------------|------------------|------------------|\n",
    "| **Momentum (SGD)** | AÄŸ Ã§ok derin deÄŸil, ancak osilasyon veya yavaÅŸ Ã¶ÄŸrenme varsa | Gradyan hÄ±zÄ±nÄ± artÄ±rÄ±r, osilasyonu azaltÄ±r |\n",
    "| **Weight Decay / L2 Regularization** | Overfitting riski yÃ¼ksek, Ã¶zellikle kÃ¼Ã§Ã¼k datasetlerde | AÄŸÄ±rlÄ±klarÄ± kÃ¼Ã§Ã¼lterek genelleme performansÄ±nÄ± artÄ±rÄ±r |\n",
    "| **Optimizer + Scheduler Kombinasyonu** | Derin veya karmaÅŸÄ±k aÄŸlarda, LRâ€™yi dinamik ayarlamak istiyorsan | Plateau veya learning rate dÃ¼ÅŸÃ¼ÅŸlerini otomatik uygular |\n",
    "| **Cyclic Learning Rate (CLR)** | Model plateauâ€™da sÄ±kÄ±ÅŸÄ±yor, Ã¶ÄŸrenme yavaÅŸ veya LR ayarlamak zor | LRâ€™i periyodik olarak deÄŸiÅŸtirir, hÄ±zlÄ± konverjans saÄŸlar |\n",
    "| **Lookahead Optimizer** | Ã‡ok derin veya karmaÅŸÄ±k aÄŸlar, gradyan stabilitesi sorunlu | AÄŸÄ±rlÄ±klarÄ± yumuÅŸatarak daha stabil konverjans saÄŸlar |\n",
    "| **CLR + Scheduler Kombinasyonu** | Ã‡ok derin aÄŸlarda ve bÃ¼yÃ¼k veri setlerinde | Ã–ÄŸrenme hÄ±zÄ±nÄ± dinamik tutarak eÄŸitim stabilitesini ve hÄ±zÄ±nÄ± artÄ±rÄ±r |\n",
    "| **Adaptive LR Optimizer (Adam, AdamW, RMSprop)** | Standart derin aÄŸlarda, Ã§oÄŸu problem | Learning rateâ€™i otomatik ayarlar, Ã§oÄŸu durumda hÄ±zlÄ± ve stabil Ã¶ÄŸrenme saÄŸlar |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Ã–zet\n",
    "\n",
    "- **KÃ¼Ã§Ã¼k ve basit aÄŸlar:** SGD veya Adam genellikle yeterli  \n",
    "- **Derin ve karmaÅŸÄ±k aÄŸlar:** Scheduler, CLR ve Lookahead gibi ileri teknikler eklenmeli  \n",
    "- **Overfitting riski varsa:** Weight decay / L2 regularization + Dropout eklemek faydalÄ±  \n",
    "- **Plateau veya stabilite sorunlarÄ± varsa:** CLR, ReduceLROnPlateau ve Lookahead kullanmak en mantÄ±klÄ±sÄ±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d729e5a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
