{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1038eb2c",
   "metadata": {},
   "source": [
    "# PyTorch Modelini Ä°leri Seviyeye GeliÅŸtirme (Advanced Enhancements)\n",
    "Bu kÄ±sÄ±mda CleanEnhancedANN modelimize ekleyebileceÄŸimiz geliÅŸmiÅŸ Ã¶zellikleri adÄ±m adÄ±m inceleyeceÄŸiz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb83e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ FarklÄ± Aktivasyon FonksiyonlarÄ±\n",
    "\n",
    "Aktivasyon fonksiyonlarÄ±, nÃ¶ronlarÄ±n Ã§Ä±kÄ±ÅŸ deÄŸerlerini dÃ¶nÃ¼ÅŸtÃ¼rerek modelin **karmaÅŸÄ±k iliÅŸkileri Ã¶ÄŸrenmesini** saÄŸlar.  \n",
    "Åu an modelimizde `LeakyReLU` kullanÄ±yoruz, ama baÅŸka seÃ§enekler de var:\n",
    "\n",
    "```python\n",
    "# ReLU\n",
    "relu = nn.ReLU()  \n",
    "# LeakyReLU\n",
    "leaky_relu = nn.LeakyReLU(0.1)  \n",
    "# ELU\n",
    "elu = nn.ELU(alpha=1.0)  \n",
    "# SELU\n",
    "selu = nn.SELU()  \n",
    "# Tanh\n",
    "tanh = nn.Tanh()  \n",
    "# Sigmoid\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aaf992",
   "metadata": {},
   "source": [
    "##  Aktivasyon FonksiyonlarÄ± â€“ Kod ve AÃ§Ä±klama Tablosu\n",
    "\n",
    "| Aktivasyon | Kod | AÃ§Ä±klama |\n",
    "|------------|-----|----------|\n",
    "| ReLU | `nn.ReLU()` | Ã‡Ä±kÄ±ÅŸ = max(0, x). Basit ve hÄ±zlÄ±, derin aÄŸlarda yaygÄ±n. Dezavantaj: negatiflerde Ã¶lÃ¼ nÃ¶ron problemi. |\n",
    "| LeakyReLU | `nn.LeakyReLU(0.1)` | Ã‡Ä±kÄ±ÅŸ = x (x>0), 0.1*x (x<0). Negatiflerde kÃ¼Ã§Ã¼k gradyan, Ã¶lÃ¼ nÃ¶ron sorununu azaltÄ±r. Mevcut modelde kullanÄ±lÄ±yor. |\n",
    "| ELU | `nn.ELU(alpha=1.0)` | x>0 â†’ x, x<=0 â†’ alpha*(exp(x)-1). Negatif deÄŸerlerde gradyan verir, Ã¶ÄŸrenme stabilitesini artÄ±rÄ±r. Derin aÄŸlarda ReLUâ€™dan daha iyi performans gÃ¶sterebilir. |\n",
    "| SELU | `nn.SELU()` | KendiliÄŸinden normalizasyon saÄŸlar (self-normalizing). Derin aÄŸlarda gradyan kaybÄ±nÄ± ve patlamasÄ±nÄ± azaltÄ±r. Ã–zellikle katman sayÄ±sÄ± fazla modeller iÃ§in gÃ¼Ã§lÃ¼. |\n",
    "| Tanh | `nn.Tanh()` | Ã‡Ä±kÄ±ÅŸ aralÄ±ÄŸÄ± [-1, 1]. Sinyali normalize eder. Dezavantaj: Ã§ok derin aÄŸlarda vanishing gradient problemi yaÅŸanabilir. |\n",
    "| Sigmoid | `nn.Sigmoid()` | Ã‡Ä±kÄ±ÅŸ aralÄ±ÄŸÄ± [0,1]. Genellikle Ã§Ä±kÄ±ÅŸ katmanÄ± iÃ§in kullanÄ±lÄ±r (binary classification). Dezavantaj: Ã§ok derin katmanlarda gradyan kaybÄ± olabilir. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed6897",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Aktivasyon FonksiyonlarÄ± â€“ KullanÄ±m SenaryolarÄ±\n",
    "\n",
    "| Aktivasyon | KullanÄ±m Durumu / Ä°htimal | Notlar |\n",
    "|------------|---------------------------|--------|\n",
    "| ReLU | Genel derin aÄŸlar, hÄ±zlÄ± ve basit modeller | Negatif deÄŸerleri sÄ±fÄ±rladÄ±ÄŸÄ± iÃ§in Ã¶lÃ¼ nÃ¶ron problemi olabilir |\n",
    "| LeakyReLU | Derin aÄŸlar, Ã¶lÃ¼ nÃ¶ron riskini azaltmak istediÄŸinde | KÃ¼Ã§Ã¼k negatif slope ile negatif gradyan saÄŸlar, mevcut modelde kullanÄ±lÄ±yor |\n",
    "| ELU | Derin aÄŸlar, daha stabil Ã¶ÄŸrenme ve negatif deÄŸerlerin Ã¶nemli olduÄŸu durumlar | ReLUâ€™dan daha iyi performans gÃ¶sterebilir, kÃ¼Ã§Ã¼k negatif gradyan verir |\n",
    "| SELU | Ã‡ok derin katmanlÄ± aÄŸlar, self-normalizing gereken durumlar | Otomatik normalize eder, gradyan kaybÄ±/patlamasÄ±nÄ± azaltÄ±r |\n",
    "| Tanh | GiriÅŸ ve Ã§Ä±kÄ±ÅŸ deÄŸerlerini normalize etmek istediÄŸinde, sinyaller [-1,1] aralÄ±ÄŸÄ±nda olmalÄ± | Vanishing gradient riski derin aÄŸlarda yÃ¼ksek |\n",
    "| Sigmoid | Binary classification Ã§Ä±kÄ±ÅŸ katmanÄ±, olasÄ±lÄ±k tahmini | Derin katmanlarda gradyan kaybÄ± olabilir, genellikle sadece Ã§Ä±kÄ±ÅŸ katmanÄ±nda kullanÄ±lÄ±r |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803898ce",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Aktivasyon FonksiyonlarÄ± â€“ Ek Notlar ve Ä°puÃ§larÄ±\n",
    "\n",
    "1. **Ã‡Ä±kÄ±ÅŸ KatmanÄ± ile Uyumluluk**\n",
    "   - `Sigmoid` â†’ binary classification iÃ§in Ã§Ä±kÄ±ÅŸ katmanÄ±\n",
    "   - `Softmax` â†’ multi-class classification iÃ§in Ã§Ä±kÄ±ÅŸ katmanÄ±\n",
    "   - Gizli katmanda Softmax kullanmak genellikle Ã¶nerilmez\n",
    "\n",
    "2. **Derinlik ve Gradyan Problemleri**\n",
    "   - ReLU ve LeakyReLU â†’ gradyan kaybÄ±nÄ± (vanishing gradient) azaltÄ±r\n",
    "   - Tanh ve Sigmoid â†’ derin aÄŸlarda gradyan kaybÄ±na yol aÃ§abilir, dikkatli kullanÄ±lmalÄ±\n",
    "\n",
    "3. **Performans ve HÄ±z**\n",
    "   - ReLU â†’ en hÄ±zlÄ±, GPUâ€™larda optimize edilmiÅŸ\n",
    "   - ELU / SELU â†’ biraz daha yavaÅŸ ama stabilite ve performans artÄ±ÅŸÄ± saÄŸlar\n",
    "\n",
    "4. **Kendi Aktivasyonunu OluÅŸturma**\n",
    "   - `nn.Module` veya `torch.autograd.Function` ile custom activation yazÄ±labilir\n",
    "   - Ã–rnek: negatif deÄŸerleri Ã¶zel bir ÅŸekilde Ã¶lÃ§eklendirmek veya yeni non-linearity denemek\n",
    "\n",
    "5. **Kombinasyonlar**\n",
    "   - Ä°leri modellerde birden fazla aktivasyon bir arada kullanÄ±labilir\n",
    "   - Ã–rnek: ilk katman ReLU, ikinci katman ELU â†’ stabilite ve hÄ±z dengesi saÄŸlanabilir\n",
    "\n",
    "\n",
    "\n",
    "ğŸ’¡ **Ã–zet:**  \n",
    "- Aktivasyon seÃ§imi **gizli katman vs Ã§Ä±kÄ±ÅŸ katmanÄ±** ve **veri tipi / problem tipi** ile doÄŸrudan iliÅŸkili.  \n",
    "- Ã‡Ä±kÄ±ÅŸ katmanÄ± iÃ§in problem tÃ¼rÃ¼ne uygun aktivasyon seÃ§mek kritik.  \n",
    "- Derinlik ve gradyan problemleri gÃ¶z Ã¶nÃ¼nde bulundurulmalÄ±.  \n",
    "- Ä°leri seviyede **custom veya kombinasyonlu aktivasyonlar** deneyebilirsin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee14f42",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a717b",
   "metadata": {},
   "source": [
    "# 2ï¸âƒ£ Residual (Skip) BaÄŸlantÄ±larÄ± ve Normalizasyon\n",
    "\n",
    "Bu bÃ¶lÃ¼mde modelin stabilitesini ve Ã¶ÄŸrenme kapasitesini artÄ±racak ileri seviye geliÅŸtirmeleri inceleyeceÄŸiz.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ Residual (Skip) BaÄŸlantÄ±larÄ±\n",
    "\n",
    "- AmaÃ§: Derin aÄŸlarda **gradyan kaybÄ±nÄ± Ã¶nlemek** ve Ã¶ÄŸrenmeyi hÄ±zlandÄ±rmak\n",
    "- MantÄ±k: Bir katmanÄ±n Ã§Ä±ktÄ±sÄ±nÄ± bir sonraki katmana **direkt ekler**\n",
    "- Ã–rnek:\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(size, size),\n",
    "            nn.BatchNorm1d(size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31a484",
   "metadata": {},
   "source": [
    "* Residual baÄŸlantÄ±larÄ± diÄŸer konularda daha net anlayacaÄŸÄ±z.ANN iÃ§in bu kadar Ã¶rnek yeterlidir diye dÃ¼ÅŸÃ¼nÃ¼yorum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3fa2b",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Batch Normalization (BatchNorm)\n",
    "\n",
    "- **AmaÃ§:** Katman Ã§Ä±kÄ±ÅŸlarÄ±nÄ± normalize ederek eÄŸitimi stabil ve hÄ±zlÄ± hale getirir\n",
    "\n",
    "- **Ã–rnek:**\n",
    "```python\n",
    "bn = nn.BatchNorm1d(64)  # 64 nÃ¶ronlu bir katman iÃ§in\n",
    "x = bn(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b10d53",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Layer Normalization (LayerNorm)\n",
    "\n",
    "- **AmaÃ§:** KÃ¼Ã§Ã¼k batch veya sequence verilerde stabilite saÄŸlamak\n",
    "\n",
    "- **Ã–rnek:**\n",
    "```python\n",
    "ln = nn.LayerNorm(64)\n",
    "x = ln(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8b3ab",
   "metadata": {},
   "source": [
    "Notlar:\n",
    "\n",
    "* Batch boyutuna baÄŸÄ±mlÄ± deÄŸildir\n",
    "\n",
    "* NLP ve sequence modellerinde sÄ±klÄ±kla tercih edilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757f870",
   "metadata": {},
   "source": [
    "---\n",
    "# 3ï¸âƒ£ Weight Initialization ve Regularization\n",
    "\n",
    "Bu bÃ¶lÃ¼mde modelin **Ã¶ÄŸrenme hÄ±zÄ±, stabilitesi ve overfitting kontrolÃ¼** iÃ§in kullanÄ±labilecek teknikleri inceleyeceÄŸiz.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ Weight Initialization\n",
    "\n",
    "- AmaÃ§: AÄŸÄ±n aÄŸÄ±rlÄ±klarÄ±nÄ± uygun bir ÅŸekilde baÅŸlatarak **hÄ±zlÄ± ve stabil Ã¶ÄŸrenme** saÄŸlamak\n",
    "- Ã–rnekler:\n",
    "\n",
    "```python\n",
    "# Xavier uniform initialization\n",
    "nn.init.xavier_uniform_(layer.weight)\n",
    "nn.init.zeros_(layer.bias)\n",
    "\n",
    "# Kaiming normal initialization (ReLU tabanlÄ± aÄŸlar iÃ§in)\n",
    "nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "nn.init.zeros_(layer.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb87759",
   "metadata": {},
   "source": [
    "Notlar:\n",
    "\n",
    "* Xavier â†’ genellikle sigmoid / tanh tabanlÄ± aÄŸlarda iyi\n",
    "\n",
    "* Kaiming â†’ ReLU / LeakyReLU tabanlÄ± aÄŸlarda daha iyi performans\n",
    "\n",
    "* Orthogonal initialization â†’ derin aÄŸlarda gradyan stabilitesi saÄŸlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3ac28",
   "metadata": {},
   "source": [
    "## ğŸ”¹ DropConnect\n",
    "\n",
    "AmaÃ§: Dropoutâ€™a benzer ama aÄŸÄ±rlÄ±klarÄ± rastgele kapatÄ±r\n",
    "\n",
    "Ã–rnek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "dropconnect = nn.Dropout(p=0.2)  # PyTorch'da DropConnect iÃ§in custom layer yazÄ±labilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48190abd",
   "metadata": {},
   "source": [
    "Notlar:\n",
    "\n",
    "* Daha agresif regularization saÄŸlar\n",
    "\n",
    "* KÃ¼Ã§Ã¼k datasetlerde overfittingâ€™i azaltmak iÃ§in tercih edilebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0ca9f",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Ã–zet Tablosu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9aa54d",
   "metadata": {},
   "source": [
    "| Teknik                 | AmaÃ§                                                  | KullanÄ±m / Notlar                                |\n",
    "| ---------------------- | ----------------------------------------------------- | ------------------------------------------------ |\n",
    "| Xavier Initialization  | Stabil Ã¶ÄŸrenme, hÄ±zlÄ± konverjans                      | Sigmoid / Tanh tabanlÄ± aÄŸlar                     |\n",
    "| Kaiming Initialization | ReLU tabanlÄ± aÄŸlarda iyi performans                   | ReLU / LeakyReLU tabanlÄ± aÄŸlar                   |\n",
    "| Dropout                | Overfittingâ€™i azaltmak                                | EÄŸitim sÄ±rasÄ±nda bazÄ± nÃ¶ronlarÄ± rastgele kapatÄ±r |\n",
    "| DropConnect            | AÄŸÄ±rlÄ±klarÄ± rastgele kapatarak overfittingâ€™i azaltmak | Ã–zellikle kÃ¼Ã§Ã¼k datasetlerde faydalÄ±             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366b803",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4ï¸âƒ£ Learning Rate ve Schedulerâ€™lar\n",
    "\n",
    "Learning Rate (LR) modelin Ã¶ÄŸrenme hÄ±zÄ±nÄ± belirler ve doÄŸru ayarlanmazsa eÄŸitim **Ã§ok yavaÅŸ veya kararsÄ±z** olabilir.  \n",
    "Schedulerâ€™lar, LRâ€™yi dinamik olarak deÄŸiÅŸtirerek konverjansÄ± hÄ±zlandÄ±rÄ±r ve stabil hale getirir.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ Sabit Learning Rate\n",
    "```python\n",
    "lr = 0.001  # Ã–rnek sabit learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded142a",
   "metadata": {},
   "source": [
    "* Basit, kÃ¼Ã§Ã¼k aÄŸlarda genellikle yeterli\n",
    "\n",
    "* Derin ve kompleks aÄŸlarda bazen stabilite sorunlarÄ± yaÅŸanabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675116a",
   "metadata": {},
   "source": [
    "## ğŸ”¹ StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2be6c65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m scheduler = torch.optim.lr_scheduler.StepLR(\u001b[43moptimizer\u001b[49m, step_size=\u001b[32m10\u001b[39m, gamma=\u001b[32m0.1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fdd5f",
   "metadata": {},
   "source": [
    "* Belirli epoch sayÄ±sÄ± sonrasÄ± LRâ€™yi Ã§arpan (gamma) ile azaltÄ±r\n",
    "\n",
    "* Ã–rnek: Her 10 epochâ€™ta LR 0.1 ile Ã§arpÄ±lÄ±r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0c309",
   "metadata": {},
   "source": [
    "## ğŸ”¹ ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe4a36",
   "metadata": {},
   "source": [
    "* Her epochâ€™ta LR Ã¼stel olarak azalÄ±r\n",
    "\n",
    "* KararlÄ± ve yumuÅŸak LR dÃ¼ÅŸÃ¼ÅŸÃ¼ saÄŸlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311d976",
   "metadata": {},
   "source": [
    "## ğŸ”¹ ReduceLROnPlateau !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eef656",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbf9ec",
   "metadata": {},
   "source": [
    "* Validasyon kaybÄ± belirli bir sÃ¼re iyileÅŸmezse LRâ€™yi azaltÄ±r\n",
    "\n",
    "* Otomatik olarak LR ayarlamak iÃ§in faydalÄ±\n",
    "\n",
    "* Ã–zellikle overfitting veya plateau durumlarÄ±nda kullanÄ±lÄ±r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccb5bd",
   "metadata": {},
   "source": [
    "| Scheduler         | AmaÃ§                           | Notlar                                        |\n",
    "| ----------------- | ------------------------------ | --------------------------------------------- |\n",
    "| Sabit LR          | Basit ve kÃ¼Ã§Ã¼k aÄŸlar           | KÃ¼Ã§Ã¼k ve stabil problem setlerinde yeterli    |\n",
    "| StepLR            | Belirli aralÄ±klarla LR dÃ¼ÅŸÃ¼rme | Epoch tabanlÄ±, gamma ile Ã§arpÄ±lÄ±r             |\n",
    "| ExponentialLR     | Ãœstel LR dÃ¼ÅŸÃ¼ÅŸÃ¼                | Daha yumuÅŸak ve stabil dÃ¼ÅŸÃ¼ÅŸ saÄŸlar           |\n",
    "| ReduceLROnPlateau | Plateau durumunda LR azaltma   | Validasyon kaybÄ± iyileÅŸmezse otomatik Ã§alÄ±ÅŸÄ±r |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e58140",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# 5ï¸âƒ£ Label Smoothing\n",
    "\n",
    "## ğŸ“Œ AmaÃ§\n",
    "- Modelin **aÅŸÄ±rÄ± gÃ¼venli tahminler yapmasÄ±nÄ±** engeller  \n",
    "- Cross-Entropy Loss yerine, sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± biraz \"yumuÅŸatÄ±r\"  \n",
    "- Bu sayede **genelleme** artar ve **overfitting** azalÄ±r  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ NasÄ±l Ã‡alÄ±ÅŸÄ±r?\n",
    "- Normalde doÄŸru sÄ±nÄ±f etiketi **%100 (1.0)** oluyordu  \n",
    "- Label smoothing ile â†’ **%90 doÄŸru sÄ±nÄ±f, %10 diÄŸer sÄ±nÄ±flara daÄŸÄ±tÄ±lÄ±r**  \n",
    "- Ã–rneÄŸin:  \n",
    "  - Normal â†’ `[0, 0, 1, 0]`  \n",
    "  - Smoothing (Îµ=0.1) â†’ `[0.033, 0.033, 0.9, 0.033]`  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ Kod Ã–rneÄŸi\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model Ã§Ä±kÄ±ÅŸÄ± (Ã¶rnek)\n",
    "outputs = torch.randn(4, 5)  # 4 Ã¶rnek, 5 sÄ±nÄ±f\n",
    "labels = torch.tensor([2, 1, 0, 3])  # gerÃ§ek etiketler\n",
    "\n",
    "# Label smoothing'li loss\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34129dc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# AÅŸaÄŸÄ±da bulunan modellerin bazÄ±larÄ± Ã¶rneklerde kullanÄ±lmÄ±ÅŸtÄ±r.Bu tÃ¼r model kullanÄ±mlarÄ±na aÅŸina olunuz.,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d6fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_MODEL(nn.Module):\n",
    "    def __init__(self, input_dim , hidden_layers=[128,64,32], dropout=0.3):\n",
    "        super(ANN_MODEL ,self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_layers:\n",
    "            self.layers.append(nn.Linear(in_dim,h_dim))\n",
    "            in_dim = h_dim\n",
    "\n",
    "        self.out = nn.Linear(in_dim,1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self , x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d4ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[256, 128, 64, 32], dropout=0.3):\n",
    "        super(AdvancedANN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_layers:\n",
    "            self.layers.append(nn.Linear(in_dim, h_dim))\n",
    "            in_dim = h_dim\n",
    "\n",
    "        self.out = nn.Linear(in_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x  # Ä°lk input residual baÄŸlantÄ±sÄ± iÃ§in\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Her 2 katmanda bir residual baÄŸlantÄ± ekle\n",
    "            if i % 2 == 1:\n",
    "                if residual.shape[1] == x.shape[1]:\n",
    "                    x = x + residual\n",
    "                residual = x  # residual'u gÃ¼ncelle\n",
    "            \n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size , output_size = 1, dp=0.3):\n",
    "        super(ANN ,self).__init__()\n",
    "\n",
    "        def linear_block(in_features , out_features , dropout):\n",
    "            return nn.Sequential(\n",
    "\n",
    "                nn.Linear(in_features , out_features),\n",
    "                nn.BatchNorm1d(out_features),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout)        \n",
    "                )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "                linear_block(input_size , 128 , dp),\n",
    "                linear_block(128,64,dp),\n",
    "                linear_block(64,32,dp),\n",
    "                nn.Linear(32,output_size)\n",
    "            )\n",
    "\n",
    "        for m in self.model:\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cd468",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
